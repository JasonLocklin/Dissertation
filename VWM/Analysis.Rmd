---
title: "Visual Working Memory and COVAT Analysis"
author: "Jason Locklin <jalockli@uwaterloo.ca>"
date: "12/1/2014"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
    toc: yes
---

This is an R Markdown document generated in R Studio. Markdown is a simple 
formatting syntax for authoring HTML, PDF, and MS Word documents. For more 
details on using R Markdown see <http://rmarkdown.rstudio.com>.

### VWM Data Import and Formating

The data file I recieved is a complicated excel file with lots of information. I
reduced it to a simple table manually and saved to a csv file for reading into 
R. 

```{r}
library(reshape2) #For "Tidying" data into useful format
library(plyr)     #For transforming data before plots and analysis
library(ggplot2)  #For producing figures
vwm <- read.csv("Data/vwm_data.csv", 
                colClasses=c("factor","factor", rep("numeric", 12)))
```


The data needs to be tidied to a convenient format (see Wickham, 2014).

```{r}
d <- melt(vwm, id.vars=c("Subj_Group", "Subj_ID"))
d = data.frame(c(d[1:2], colsplit(d$variable, "_item[s]?.", c("Number_of_Items","variable")), d[4]))
head(d, n=3)
```


A quick summary of the data for each group.
```{r}
acast(d, Subj_Group ~ variable ~ Number_of_Items , fun=mean)
```

#### What it looks like:

1. Looking at precision, Patients generally do worse than Older Controls, who
generally do worse than Young adults. Each group generally does worse as more 
targets are added.

2. pTarget: Patents are averaging lower probabilities of selecting the target 
color even in the case of no distractors. Distractors may be effective in the 
healthy groups, but they do quite well.

3. The healthy groups seem to rarely select a distractor color, and seem to be 
guessing when they get it wrong. The Patients might be selecting distractor 
colors as often as they guess, indicating a possible lack of binding color to 
spatial location.

### Research Questions:

1. Are the patients able to encode and recall colour? 
2. Is there a deficit of colour WM?
3. Are the patients able to bind/recall location and colour of two targets?
4. What about 3?
5. Again, is there a deficit of this binding?
6. Are the patients mis-binding color/location in the 2/3 condition? or are they
   forgetting the stimuli entirely? (i.e., guessing or NonTarget selection).
   

### Are the patients able to encode and recall colour? A deficit?

Just looking at the "one target" case, were patients able to reliably report the
colour of the target.


Create summary data:
```{r}
simple <- d[d$variable == "precision" & d$Number_of_Items =="one",]

cdata <- ddply(simple, c("Subj_Group", "variable"), summarise,
               N    = length(value),
               Mean = mean(value),
               sd   = sd(value),
               se   = sd / sqrt(N) )
head(cdata, n=3)
```



Create a plot of means, and a seperate point plot to see outliers:
```{r }
p <- ggplot(cdata, aes(x = Subj_Group, y = Mean)) 
p + geom_bar(stat = "identity", width=0.5) +
    geom_errorbar(aes(ymin=Mean-se, ymax=Mean+se), width=.2) + 
    xlab("") +
    ylab("Precision (SE)") + 
    ggtitle("Response Precision for Single Target Condition") +
    scale_y_continuous(breaks=0:20*4) +
    theme_gray()

ggplot(simple, aes(x = Subj_Group, y = value))  + geom_jitter(position = position_jitter(width = .1))
```

Both older groups seem to have a single extreme case, and in both cases, that 
point will drastically increase the mean precision of the group. However, 
removing those two points would not make the Patient group unique. At best, it 
would make the two older groups different from the Young Adults (they would be
less precise). There is no point in further analysis of this point as I'm not
concerned with differences due only to aging.





#### Looking at probability of target selection, a derivative of precision.

Create summary data:
```{r}
simple <- d[d$variable == "pTarget" & d$Number_of_Items =="one",]

cdata <- ddply(simple, c("Subj_Group", "variable"), summarise,
               N    = length(value),
               Mean = mean(value),
               sd   = sd(value),
               se   = sd / sqrt(N) )
head(cdata, n=3)
```

Create a plot of means, and a seperate point plot to see outliers:
```{r fig.height=3}
p <- ggplot(cdata, aes(x = Subj_Group, y = Mean)) 
p <- p + geom_bar(stat = "identity", width=0.5) +
    geom_errorbar(aes(ymin=Mean-se, ymax=Mean+se), width=.2) + 
    xlab("") +
    ylab("Probability (SE)") + 
    ggtitle("Probability of Correct Target Selection for Single Target Condition") 

p + theme_gray()
```

Trying out some other styles. Choose one:
```{r fig.width=7, fig.height=3}
p + theme_bw()
p + theme_linedraw()
p + theme_light()
p + theme_minimal()
p + theme_classic()


ggplot(simple, aes(x = Subj_Group, y = value))  + geom_jitter(position = position_jitter(width = .1))
```

Perform an ANOVA:
```{r}
fit <- aov(value ~ Subj_Group, data=simple) 
layout(matrix(c(1,2,3,4),2,2)) # optional layout
plot(fit) # diagnostic plots
```

Anova table:
```{r}
summary(fit)
TukeyHSD(fit)
```

#### Save the pTarget data for later contrast
```{r}
VWM_SINGLE_P <- d[d$variable == "pTarget" & d$Subj_Group == "Patient" & 
                    d$Number_of_Items == "one",][,c(2,4,5)]
VWM_SINGLE_P$variable <- "VWM_Single"
VWM_SINGLE_P$value <- 1 - VWM_SINGLE_P$value #Reverse code so + is more neglecting
names(VWM_SINGLE_P)[1] <- "Subject"
```


### Are the patients able to bind/recall location and colour?

The two and three target conditions are really a different experiment from the 
single target case. The participant needs to bind the locations of the targets 
with the colour of each, and when presented with the cue, recall which color
was at that location. Failure can occur because 1. The color for that location 
cannot be recalled (Guessing), or 2. The color from another location was 
miss-recalled at that location (PNonTarget).

"Guessing", "pNonTarget", and "pTarget" add to 1, so are dependant/represent
2 degrees of freedom.

The omnibus model effectively has 3 Groups, two conditions, two outcomes. 
However, the two outcomes are effectively answers to two different questions, 
so I will not bother with a mulitvariate ANOVA.

#### Q. 1: Do the groups differ in how often they mis-bind colour and location
(I.e., probability of non-target colour)

Create summary data:
```{r}
simple <- d[d$variable == "pNonTarget" & d$Number_of_Items !="one",]

cdata <- ddply(simple, c("Subj_Group", "Number_of_Items","variable"), summarise,
               N    = length(value),
               Mean = mean(value),
               sd   = sd(value),
               se   = sd / sqrt(N) )
cdata
```

Create a plot of means, and a seperate point plot to see outliers:
```{r fig.height=3}
p <- ggplot(cdata, aes(x = Subj_Group, y = Mean, fill = Number_of_Items)) 
p <- p + geom_bar(stat = "identity", width=0.5,position=position_dodge(.6)) +
    geom_errorbar(aes(ymin=Mean-se, ymax=Mean+se), width=.2, position=position_dodge(.6)) + 
    xlab("") +
    ylab("Probability of Non-Target selection (SE)") + 
    ggtitle("Mis-Binding Likelyhoods for Multi-Target Conditions") 

p + theme_gray()
```

Plot the points (scatterplot)
```{r}
p <- ggplot(simple, aes(x = Subj_Group, y = value, fill = Number_of_Items))  
p + geom_jitter(position = position_jitter(width = .1))
```

This data is quite strikingly skewed. Plot histograms (density plot) to see:
```{r}
ggplot(simple, aes(x = value,  fill = Subj_Group)) + geom_density(alpha=I(.5),)
```

The patient data has a uniform distribution (over the limited range of 0-1 
probaility), while the two healthy groups are effectively at "floor". Because of
this, it looks like it's impossile to transform the data into a sufficiently normal
distribution. Taking the quad-root seems to produce the best result, but it's 
still not good enough:

```{r}
ggplot(simple, aes(x = (value)^(1/4),  fill = Subj_Group)) + geom_density(alpha=I(.5),)
```


Because of these extreme distributions, the best way to model this would be with 
a non-parametric test. Unfortunately, non-parametric tests of hierarchical data
(the between-subject grouping factor X the within-subject num-targets factor),
are not well established (at least in the frequentist world). Perhapse this data
would best be analized with a bayesian model. For now, there are two simple solutions,
treat the two conditions seperately (and use a bonferonni correction *if* only
one condition is significant), or collapse the data across both conditions and 
lose any ability to compare the 2 and 3 target conditions.

Two target condition:
```{r}
simple <- d[d$variable == "pNonTarget" & d$Number_of_Items =="two",]
summary(aov(value^(1/4)~Subj_Group, data=simple)) #Transformed data
kruskal.test(value ~ Subj_Group, data=simple)     #Non-parametric test
```

Three target condition:
```{r}
simple <- d[d$variable == "pNonTarget" & d$Number_of_Items =="three",]
summary(aov(value^(1/4)~Subj_Group, data=simple)) #Transformed data
kruskal.test(value ~ Subj_Group, data=simple)     #Non-parametric test
```

Not useful.

Collapsing across the two conditions:
```{r}
simple <- d[d$variable == "pNonTarget" & d$Number_of_Items !="one",]
simple <- dcast(simple, Subj_Group + Subj_ID ~ variable, fun=mean) #take mean of 2 pts 
fit <- aov(pNonTarget^(1/4)~Subj_Group, data=simple)  #Transformed data
summary(fit)
kruskal.test(pNonTarget ~ Subj_Group, data=simple)     #Non-parametric test
```

Both the tranformed-data ANOVA, and non-parametric test are significant when 
collapse the two conditions. Next, making the necissary multiple comparisons:


```{r}
TukeyHSD(fit) # Tukey of the transformed data (two-tail)
```



For a non-parametric set of contrasts, I use the nparcomp package. Here I do 
non-parametric version of single-sided Tukey contrasts. I use single sided tests
because of the a priori assumption that both neglect and age will, if anything, 
make people
less accurate (i.e., the Young controls will be best, and the Patient group will
be worst). Using a single-tailed version regains some of the power lost by 
using a non-parametric test.

Note I run a Tukey test first, just to pull out the contrast matrix
and flip the necissary values to meet the a priori claim. Then I run a second
model with these "costom" contrasts. This is just because the software
does not allow me to specifiy the order of the directional contrasts in the 
Tukey test, and by chance, two of the contrasts are in the wrong direction.

```{r}
library(nparcomp)
fit <- nparcomp(pNonTarget ~ Subj_Group, data=simple,asy.method = "mult.t",
            type = "Tukey",alternative = "greater", 
            plot.simci = TRUE, info = TRUE)
ContMatrix <- fit$Contrast * c(1,-1,-1) 
fit <- nparcomp(pNonTarget ~ Subj_Group, data=simple,asy.method = "mult.t",
            type="UserDefined",alternative = "greater", 
            contrast.matrix = ContMatrix,
            plot.simci = TRUE, info = TRUE)

summary(fit)
```

Now we see the patient group is significantly worse (higher probability of 
selecting non-target colour) than both control groups. The older control is not
worse than the Young controls. In other words, no evidence that age is a factor,
but strong evidence that neglect is.

Re-plotting these means for publication:

```{r fig.height=3}
cdata <- ddply(simple, .(Subj_Group), summarise,
               N    = length(pNonTarget),
               Mean = mean(pNonTarget),
               sd   = sd(pNonTarget),
               se   = sd / sqrt(N) )

p <- ggplot(cdata, aes(x = Subj_Group, y=Mean)) 
p <- p + geom_bar(stat = "identity", width=0.5,position=position_dodge(.6)) +
    geom_errorbar(aes(ymin=Mean-se, ymax=Mean+se), width=.2, position=position_dodge(.6)) + 
    xlab("") +
    ylab("Probability of Selection (SE)") + 
    ggtitle("Non-Target Colour Selection") 

p + theme_gray()
```

TODO: Should add bars and stars indicating that Patient differs statistically
from the other two groups, but they do not significantly differ from each other.

#### Save the pTarget data for later contrast
```{r}
VWM_P <- simple[simple$Subj_Group == "Patient",][,2:3]
names(VWM_P) <- .(Subject, value)
VWM_P$variable <- "VWM_Multi"
VWM_P <- rbind(VWM_SINGLE_P, VWM_P)
```


### Do they fail to recall target colours in multi-target condition?

Create summary data:
```{r}
simple <- d[d$variable == "guessing" & d$Number_of_Items !="one",]

cdata <- ddply(simple, c("Subj_Group", "Number_of_Items","variable"), summarise,
               N    = length(value),
               Mean = mean(value),
               sd   = sd(value),
               se   = sd / sqrt(N) )
cdata
```

Create a plot of means, and a seperate point plot to see outliers:
```{r fig.height=3}
p <- ggplot(cdata, aes(x = Subj_Group, y = Mean, fill = Number_of_Items)) 
p <- p + geom_bar(stat = "identity", width=0.5,position=position_dodge(.6)) +
    geom_errorbar(aes(ymin=Mean-se, ymax=Mean+se), width=.2, position=position_dodge(.6)) + 
    xlab("") +
    ylab("Probability of Guessing (SE)") + 
    ggtitle("Guessing Likelyhoods for Multi-Target Conditions") 

p + theme_gray()
```

Plot the points (scatterplot)
```{r}
p <- ggplot(simple, aes(x = Subj_Group, y = value, fill = Number_of_Items))  
p + geom_jitter(position = position_jitter(width = .1))
```

The data is far less skewed than what we saw with pNonTArget. 
Plot histograms (actually a density plot) to see:
```{r}
ggplot(simple, aes(x = value,  fill = Subj_Group)) + geom_density(alpha=I(.5),)
```


A square-root transform completely eliminates the skew, though the lack of tails
causes the data to remain extremely non-normal:
```{r}
ggplot(simple, aes(x = (value)^(1/2),  fill = Subj_Group)) + geom_density(alpha=I(.5),)
```


Again, non-parametric testing would be safest.

Two target condition:
```{r}
simple <- d[d$variable == "guessing" & d$Number_of_Items =="two",]
summary(aov(value^(1/2)~Subj_Group, data=simple)) #Transformed data
kruskal.test(value ~ Subj_Group, data=simple)     #Non-parametric test
```

Three target condition:
```{r}
simple <- d[d$variable == "guessing" & d$Number_of_Items =="three",]
summary(aov(value^(1/2)~Subj_Group, data=simple)) #Transformed data
kruskal.test(value ~ Subj_Group, data=simple)     #Non-parametric test
```

Again, not useful.

Collapsing across the two conditions:
```{r}
simple <- d[d$variable == "guessing" & d$Number_of_Items !="one",]
simple <- dcast(simple, Subj_Group + Subj_ID ~ variable, fun=mean) #take mean of 2 pts 
fit <- aov(guessing^(1/2)~Subj_Group, data=simple)  #Transformed data
summary(fit)
kruskal.test(guessing ~ Subj_Group, data=simple)     #Non-parametric test
```

As suspected from the plots, the groups don't seem to differ at all at the 
likelyhood of guessing. Just for completeness, here is the exact same multicomparison
analysis as was done with pNonTarget above:

```{r}
TukeyHSD(fit) # Tukey of the transformed data (two-tail)
library(nparcomp)
fit <- nparcomp(guessing ~ Subj_Group, data=simple,asy.method = "mult.t",
            type = "Tukey",alternative = "greater", 
            plot.simci = TRUE, info = TRUE)
ContMatrix <- fit$Contrast * c(1,-1,-1) 
fit <- nparcomp(guessing ~ Subj_Group, data=simple,asy.method = "mult.t",
            type="UserDefined",alternative = "greater", 
            contrast.matrix = ContMatrix,
            plot.simci = TRUE, info = TRUE)

summary(fit)
```

And a plot of the means:

```{r fig.height=3}
cdata <- ddply(simple, .(Subj_Group), summarise,
               N    = length(guessing),
               Mean = mean(guessing),
               sd   = sd(guessing),
               se   = sd / sqrt(N) )

p <- ggplot(cdata, aes(x = Subj_Group, y=Mean)) 
p <- p + geom_bar(stat = "identity", width=0.5,position=position_dodge(.6)) +
    geom_errorbar(aes(ymin=Mean-se, ymax=Mean+se), width=.2, position=position_dodge(.6)) + 
    xlab("") +
    ylab("Probability of Guessing (SE)") + 
    ggtitle("Guessing Colour Selection") 

p + theme_gray()
```

From the above we could suspect there may be an age effect, even though I havn't
found statistical justification. From the plots, I suspect it's possible to 
pull out an age effect. What is obvious, though, is that there doesn't appear to
be a striking influence of neglect, like with pNonTarget.

To demonstrate the lack of a difference numerically:

```{r}
c<- t.test(guessing~ Subj_Group, data=simple[simple$Subj_Group != "Young Adult",])

c<- t.test(simple[simple$Subj_Group == "Older Control",]$guessing)$conf.int
c
c[2]-c[1]
```

The *maximum* difference between the two groups, based on a 95 percent confidence
interval, is 0.11, while the CI for the population of older controls it's self 
is more than twice that. *If* there is a real effect of neglect on the probability
of guessing, it is going to be dwarfed by inter-subject variability. 

I could do a more detailed power analysis here if necissary.

#### Prep VWM data for contrasts between tasks


### COVAT data import
```{r}
covat <- read.csv("Data/covat_data.csv", na.strings=0) # No resp. was coded as 0
covat$Subject <- factor(covat$Subject)
```

The data needs to be tidied to a convenient format (see Wickham, 2014).

```{r}
RAW_COVAT <- melt(covat, id.vars=c("Subject", "Group"))
RAW_COVAT = data.frame(c(RAW_COVAT[1:2], 
                         colsplit(RAW_COVAT$variable, "_", c("VF", "Cue", "SOA")), 
                         RAW_COVAT[4]))
head(RAW_COVAT, n=3)
```

The analysis here is modeled after Striemer(2007). Cue-effect-sizes for leftward
and rightward shifts of attention are calculated:
For

* Leftward shift cost (Cue on the right): L_IV - R_V
* Rightward shift cost (Cue on the left): R_IV - L_V

```{r}
covat$CES.L_50 <- covat$L_IV_50 - covat$R_V_50
covat$CES.L_150 <- covat$L_IV_150 - covat$R_V_150
covat$CES.R_50 <- covat$R_IV_50 - covat$L_V_50
covat$CES.R_150 <- covat$R_IV_150 - covat$L_V_150
```

Tidying the dataset

```{r}
CES <- melt(covat, id.vars=c("Subject", "Group"), 
            measure.vars=.(CES.L_50, CES.L_150, CES.R_50, CES.R_150))
CES = data.frame(c(CES[1:2], colsplit(CES$variable, "_", c("VF", "SOA")),
                   CES[4]))
head(CES, n=3)
```

Now plot the CES data:

```{r}
p <- ggplot(CES, aes(x = Group, y=value, fill=VF)) 
p + geom_boxplot() +      
  xlab("SOA") +
  ylab("Reaction Time (ms)") + 
  ggtitle("Cue-Effect-Size (CES) of Both Groups in Each Condition") +
  geom_jitter(position = position_jitter(width = .25), width=10) + 
  facet_grid(. ~ SOA)
```

There is quite a striking assymetry for the Neglect group, though it appears to 
be more solid in the 50ms SOA condition. Differece scores beween leftward and 
rightward cue effect sizes would provide a measure of assymetrical deployment 
of attention (or attention-assymetry, aa).
This can be used to compare the two groups easily, as well as
correlate with the VWM results (see next section).

#### Attention Assymetry (CES Left-Right Difference score)
```{r}
covat$aa_50 = covat$CES.L_50 - covat$CES.R_50
covat$aa_150 = covat$CES.L_150 - covat$CES.R_150
AA <- melt(covat, id.vars=c("Subject", "Group"), 
            measure.vars=.(aa_50, aa_150))
AA = data.frame(c(AA[1:2], colsplit(AA$variable, "_", c("Assymetry", "SOA")),
                   AA[4]))
head(AA, n=3)
```

Plottig these new scores:
```{r}
p <- ggplot(AA, aes(x = Group, y=value)) 
p + geom_boxplot() +      
  xlab("SOA") +
  ylab("Assymetry (Positive is a higher cost of leftward reorienting)") + 
  ggtitle("Attention Assymetry of Both Groups in Each Condition") +
  geom_jitter(position = position_jitter(width = .25), width=10) +
  geom_hline(yintercept=0, size=2, alpha=I(.5)) +
  facet_grid(. ~ SOA)
```

A density plot of the distributions:
```{r}
ggplot(AA, aes(x = value,  fill = Group)) + geom_density(alpha=I(.5),)
```


Before running any statistical tests, it must be noted that 3 patients could 
not orient toward the left at all, and are encoded as NAs.  I'm not sure there
is anything I can do with them. It's obviously an effect of Neglect, but it's 
not necissarily related in any way to the cuing effect size measured here.
Perhapse, when comparing with VWM, I can encode them as 3 levels. A median split
on AA giving low and high bias, and the NAs giving a third level of attentional
bias.

Regardless, dropping 3 patients from the dataset unbalances the anova. 

Perform an ANOVA:
```{r}
require(ez)
AA$SOA <- factor(AA$SOA)
ezANOVA(AA[!is.na(AA$value),], wid = .(Subject), dv = .(value), 
        within = .(SOA), 
        between = .(Group))

# Repeating with type 3 SS
ezANOVA(AA[!is.na(AA$value),], wid = .(Subject), dv = .(value), 
        within = .(SOA), 
        between = .(Group), type=3)
```

Group effect is non-significant, but p<.1, regardless of sum of squares 
calculation used. Encoding SOA as a covariate doesn't make a difference (not
printed). Taking a look at the means plot:


```{r}
ezPlot(AA[!is.na(AA$value),], wid = .(Subject), dv = .(value), 
        within = .(SOA), 
        between = .(Group),
       x= .(Group),
       split =.(SOA))
```

It makes sense why only group is close to significant. Note that the Older Controls
are centered right on zero -i.e., no spatial bias whatsoever.

Post hoc tests need to be performed in the context of the non-significant 
ANOVA. Single-tailed tests are performed because it is assumed neglect patients
will be more laterally biased (re: Striemer2007).

```{r}
t.test(.~Group, alternative="less",
       data = dcast(AA, Subject + Group ~ ., mean))
t.test(.~Group, alternative="less",
       data = dcast(AA, Subject + Group ~ ., subset=.(SOA==50), mean))
t.test(.~Group, alternative="less",
       data = dcast(AA, Subject + Group ~ ., subset=.(SOA==150), mean))
```

There isn't really good evidence that the Patient group is different as a whole.
Looking at the assymetry plots, it looks like some in this group are strongly 
affected, while others are not. I plan on splitting up the group based on their
clinical tests to see if that provides a way of seperating these two types.

For now, let's see if the group is significantly biased:

```{r}
q <- dcast(AA, Subject ~ ., subset=.(Group=="Patient" & !is.na(value)),   mean)
t.test(q[2], alternative="greater")

q <- dcast(AA, Subject ~ ., subset=.(Group=="Patient" & 
                                                SOA==50 & 
                                                !is.na(value)),   mean)
t.test(q[2], alternative="greater")
q <- dcast(AA, Subject ~ ., subset=.(Group=="Patient" & 
                                                SOA==150 & 
                                                !is.na(value)),   mean)
t.test(q[2], alternative="greater")
```

Again, not significant, even without accounting for multiple comparisons.

#### CES Left scores
Re-doing the above analysis with just the CES-Left scores, as this may be a better
measure of the disengagement deficit and it also more closely follows what was
used in Striemer2007.


Doing the ANOVA again. Again, there are only 5 in the Patient group with scores,
so it's likely not useful.

Perform an ANOVA:
```{r}
require(ez)
CES$SOA <- factor(CES$SOA)
CES_short <- CES[!is.na(CES$value) & ( CES$VF == "CES.L" ),]
ezANOVA(CES_short, wid = .(Subject), dv = .(value), 
        within = .(SOA), 
        between = .(Group))

# Repeating with type 3 SS
CES_short <- CES[!is.na(CES$value) & ( CES$VF == "CES.L" ),]
ezANOVA(CES_short, wid = .(Subject), dv = .(value), 
        within = .(SOA), 
        between = .(Group))
```
As expected.


```{r}
ezPlot(CES_short, wid = .(Subject), dv = .(value), 
        within = .(SOA), 
        between = .(Group),
       x= .(Group),
       split =.(SOA))
```


Post hoc tests need to be performed in the context of the non-significant 
ANOVA. Single-tailed tests are performed because it is assumed neglect patients
will be more laterally biased (re: Striemer2007).

```{r}
t.test(.~Group, alternative="less",
       data = dcast(CES_short, Subject + Group ~ ., mean))
```


Again, Nonzero for group?

```{r}
q <- dcast(CES_short, Subject ~ ., subset=.(Group=="Patient"),   mean)
t.test(q[2], alternative="greater")

```
Huh, significant. Perhapse the leftward CES is, indeed, a more sensitive measure
of the neglect attention defict. Note that this would not reach significance if
all the means were compared and some correction for multiple comparisons made.
It can be argued, though, that this is a particularily key contrast, and can be
done on it's own. Regardless, working from a group of 5 is not going to be sufficient
for most group-level statistics.

Future work: Single case study statistics could be made comparing each of the 5 individuals 
to the normative group. It's quite obvious from the plots that several of the 
Patients would score significantly outside of the normal range.


#### Prepare for contrasting the effects with Clinical/VWM measurse
For comparing with other tests:
```{r}
names(CES)[3] <- "variable"
names(AA)[3] <- "variable"
CES_AA <- rbind(CES,AA)
COVAT_P <- dcast(CES_AA, Subject + variable ~ .,
                 subset=.(Group=="Patient" & variable != "CES.R"),   mean)
names(COVAT_P)[3] <- "value"
```


```{r}

BOUND <- rbind(COVAT_P, VWM_P)
```


### Clinical Measures

Three clinical measures were used: Star Cancellation, figure copying and Line
bisection. Only one participant scored negatively on figure copying, and that 
participant also scored low on the other two tests. As a result, the figure 
copying variable is not useful statistically, and only the other two are used.

```{r}
C.TESTS <- read.csv("Data/demographics_data.csv")
C.TESTS$Subject <- factor(C.TESTS$Subject)
names(C.TESTS) <- .(Subject, Group, Age, Sex, Handedness, Stars, Copying, Bisection)
t.test(Age ~ Group, data=C.TESTS)
TESTS_P <- C.TESTS[C.TESTS$Group == "Patient",][,c(1,6,8)]
```


### Correlation between VWM performance and Covat performance

The chapter hypothesis claims that both VWM (perceptual, new) as well as
Covat (attentional, established) will be degraded in neglect  **and** those 
deficits will not be strongly related. For this a simple "performance" measure
needs to be calculated for each task and participant. Then, a correlation tested
for.

The following numeric variables are compared:
1. Clinical measures of neglect 
2. Covat "AA" -attention assymetry score
3. Covat CES.L -Cue effect size for reorienting leftward
4. Probability of target color selection in 1-target VWM (score reversed)
4. Probability of non-target selection in multi-target VWM.

High level overview:
```{r}
COR <- dcast(BOUND, Subject ~ variable + .)
COR <- cbind(COR, TESTS_P, by="Subject")[,c(1:5,7,8)]
ezCor(COR[,c(2:7)])
```

The NAs are a problem to deal with. Just looking at the 
tests for those patients able to do the COVAT (remember, this is only
5 participants). Also, note that the three participants unable to reorient 
leftward scored very high on the clinical tests of neglect. Removing them
is removing the most neglecting individuals, and will seriously hamper 
any correlation calculation.

```{r}
ezCor(COR[!is.na(COR$aa),][,c(2:7)])
```


Now, just for completeness, Looking at the VWM + Clinical tests for everyone in 
the Patient group (this is exactly the same as the bottom for values from the 
origional plot.)
```{r}
ezCor(COR[,c(4,5,6,7)])
```

I suspect that because of the lack of data points for all but 5 participants
in the COVAT task, I will be restricted to individual comparisons rather than
group contrasts.


```{r}
summary(COR)
COR
```

