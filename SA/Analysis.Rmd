---
title: "Saccadic Adaptation"
author: "Jason Locklin"
date: "2/9/2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

This analysis code is present for transparency only, not for educational or reference use. The code herein comprises work over a long period of time, where I learned to use several tools, and new tools became available. As a result, there are inconsistencies in style, lots of inelegance, and inefficient methods. 


## Load and prepare the data:

```{r}
library(reshape2)
library(plyr)
library(ggplot2)

#override one-sample t-test for use here that prints output for writeup.
t.test.md <- function(d){
  t <- t.test(d)
  print(paste("$t(", t$parameter, ") =", round(t$statistic,2), "$, $\text{p} =", round(t$p.value,3), "$"))
  return(t)
}


# Read in data files
LD <- read.csv("data/LandmarkData.csv")
BD <- read.csv("data/BisectionData.csv")
AD <- read.csv("data/AdaptationEffect.csv", na.strings="")
AD_TT <- read.csv("data/AdaptationEffect_TT.csv", na.strings="")
PAR <- read.csv("data/AdaptationData.csv")[2:10]
      # this file contains less processed adaptation data, but we load it
      # here for the experimental parameters it contains for each sj.

#Reformat them in to a "long" format where they can easily be concatinated

# Participants who did -dir condition, or removed for other reasons.
#  H72 -glitch. Random direction from block to block
#  99999 - me.
#  102 - only did one block.
bad <- c('1004', '102', '106', '107', 'H62', 'H63', 'H65', 'H72', 'H66', '99999')

#LD
LD$block <- factor(LD$block)
names(LD) <- c("Sj", "block", "Landmark")
LD <- melt(LD, id=c("Sj","block"))
LD <- LD[LD$value < 4,]
LD <- LD[!(LD$Sj %in% bad),] #rm -dir people

#BD
BD$block <- factor(BD$block)
names(BD) <- c("Sj", "block", "Bisection")
BD <- melt(BD, id=c("Sj","block"))
BD <- BD[BD$value < 4,]
BD <- BD[!(BD$Sj %in% bad),]

# AD
AD <- melt(AD, id="Subject", na.rm=TRUE)
names(AD) <- c("Sj", "block", "Adaptation")
levels(AD$block) <- c(0,1,2,3)
AD <- melt(AD, id=c("Sj","block"))
AD <- AD[!(AD$Sj %in% bad),]

AD_TT <- AD_TT[!(AD_TT$Subject %in% bad),]
## Processing and Checking the saccadic Adaptation:

# PAR
# Need to go back to processpickles.py and fix this, but for now, do it here:
# Recode "Gain" as dir(ection) of saccade, and create a new "gain" that
# encodes the magnitude purturbation (and hence the saccadic gain).
PAR['Dir'] <- PAR['Gain']
g <- PAR['Gain']
PAR['Gain'] <- round(( g*PAR$Tc - g*PAR$Tb ) / (g*PAR$Tb - g*PAR$Ta), 1)
PAR$Gain[is.na(PAR$Gain)] <- 0  #Na's generated above for Tb == Ta
# Now, +Gain should increase saccadic length, - decrease it.
# +Dir indicates rightward initial saccade, - leftward.
# A combination of Gain and Dir should impact judgement of center.


#To check the data
mlt <- melt(PAR, id=c("Subject","Block"), measure.vars = c("Gain","Dir"), na.rm=TRUE)
print(dcast(mlt, Subject ~ Block + variable, mean), digits=1)
print(dcast(mlt, Subject ~ Block + variable, sd), digits=1)
#Odd, remove subject H72, a no-purturbation case, had randomized direction. Must have been a glitch.


#Now format the data so it's "tidy":
d <- dcast(mlt, Subject + Block ~  variable, median) 
PAR <- melt(d, id=c("Subject","Block"), measure.vars = c("Gain","Dir"), na.rm=TRUE)
names(PAR) <- c("Sj", "block", "variable", "value")
rm(d,g,mlt)
```



Note that while I subtracted each adaptation effect from the baseline, the 
landmark and line bisection data are not difference scores (block
0 is baseline.)

### Before computing change scores, are they correlated?
```{r}
t <- rbind(BD,LD)
t <- t[t$Sj != "H57",]
t <- dcast(t, Sj ~ variable, mean)
t <- t[!is.na(t$Bisection),]
summary(t)
sd(t$Bisection)
t.test.md(t$Bisection)
sd(t$Landmark)
t.test.md(t$Landmark)
mean(abs(t$Bisection))
mean(abs(t$Landmark))
cor.test(t$Bisection, t$Landmark)

t <- rbind(BD,LD)
t <- dcast(t, Sj ~ variable, subset=.(block==0), mean)
t <- t[!is.na(t$Bisection),]
summary(t)
t.test.md(t$Bisection)
t.test.md(t$Landmark)
cor.test(t$Bisection, t$Landmark)

```



## Now compute change scores

```{r}
# Compute change from baseline for each block.
# Note that values are in cm. Linelength is available in the parameters.
#Landmark

# Also note that for Sj's 100, and H20-H57 there was a bug that prevented a 
# final LB/LT after the final adaptation block. 
t <- dcast(LD, Sj ~ block +variable, fun=mean)
t[[3]] = t[[3]] - t[[2]] 
t[[4]] = t[[4]] - t[[2]] 
t[[5]] = t[[5]] - t[[2]] 
t[[6]] = t[[6]] - t[[2]] 
t<- t[,c(1,3,4,5,6)]
names(t) <- .(Sj,0,1,2,3)
t <- melt(t, id.vars="Sj")  #I'm sleepy, can't figure out a non-stupid way of doing this
names(t) <- .(Sj, block, value)
t$variable <- "Landmark"
head(t)
LD <- t
rm(t)

#Bisection
t <- dcast(BD, Sj ~ block +variable, fun=mean)
t[[3]] = t[[3]] - t[[2]] 
t[[4]] = t[[4]] - t[[2]] 
t[[5]] = t[[5]] - t[[2]] 
t[[6]] = t[[6]] - t[[2]] 
t<- t[,c(1,3,4,5,6)]
names(t) <- .(Sj,0,1,2,3)
t <- melt(t, id.vars="Sj")  #I'm sleepy, can't figure out a non-stupid way of doing this
names(t) <- .(Sj, block, value)
t$variable <- "Bisection"
head(t)
BD <- t
rm(t)
```





```{r}
#Now, put them all together for analysis:
d <- rbind(LD,BD,AD,PAR)
d <- d[!is.na(d$value),] # Remove 2 NAs for participant who didn't do 4th session



# Dir and gain should be identifiers
d <- melt(dcast(d, Sj + block ~ variable, mean), id=c("Sj","block","Gain","Dir"), na.rm=TRUE)
#Collapse Gain down into a 3-level factor
g <- d$Gain * 0
g[d$Gain > 0] <- "0.3"
g[d$Gain < 0] <- "-0.3"
d$Gain <- factor(g)

d$Dir <- factor(d$Dir)

d <- d[d$Sj != "H72" & d$Sj != "99999" & d$Sj != "102", ]
summary(d)
```

Looking at adaptation:
1. Was it effective as a whole
2. Select the people that showed reliable adaptation for further analysis.


For 1:

## Effectiveness of Adaptation

###First, test-trials only:

```{r}
t.test.md(AD_TT$X0)
t.test.md(AD_TT$X1)
t.test.md(AD_TT$X2)
t.test.md(AD_TT$X3)
t.test.md(AD_TT$X0 + AD_TT$X1 + AD_TT$X2 + AD_TT$X3)



t2 <- t.test(AD_TT$X0)
t <- data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate)
t2 <- t.test(AD_TT$X1)
t <- rbind(t, data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate))
t2 <- t.test(AD_TT$X2)
t <- rbind(t, data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate))
t2 <- t.test(AD_TT$X3)
t <- rbind(t, data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate))
```


Because test-trials are so limited, many participants are missing scores for
some, if not all blocks. Now that it's been shown that the SA is "working"
overall, and not an artifact of mid-saccadic changes in tragectory,
I use data from all trials below and hereafter.

```{r}
# Split the group on the four "gain" conditions

t1 <- dcast(d, Sj ~ block + variable, 
            subset=.(variable == "Adaptation" & Gain == -0.3), fun=mean) 
t.test.md(t1[[2]])
t.test.md(t1[[3]])
t.test.md(t1[[4]])
t.test.md(t1[[5]])

# t2 <- dcast(d, Sj ~ block + variable, 
#             subset=.(variable == "Adaptation" & Gain == -0.2), fun=mean) 
# t.test.md(t2[[2]])
# t.test.md(t2[[3]])
# t.test.md(t2[[4]])
# t.test.md(t2[[5]])

t2 <- t.test(t1[[2]])
tn <- data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate)
t2 <- t.test(t1[[3]])
tn <- rbind(tn, data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate))
t2 <- t.test(t1[[4]])
tn <- rbind(tn, data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate))
t2 <- t.test(t1[[5]])
tn <- rbind(tn, data.frame(t2$statistic, t2$parameter, t2$p.value, t2$estimate))


```

```{r}
d_tt <- melt(AD_TT, id="Subject", na.rm=TRUE)
names(d_tt) <- c("Sj","block","value")
levels(d_tt$block) <- c(1,2,3,4)

#Test trials
adn1 <- ddply(d_tt[d$variable == "Adaptation" & d$Gain == -0.3,],
             .(block), summarise, 
        N    = length(value),
        means = round(mean(value),2),
        sds = round(sd(value), 2),
        se   = sds / sqrt(N) )[1:4,]
adn1$type <- 'TestTrials'

#all trials
levels(d$block) <- c(1,2,3,4)
adn2 <- ddply(d[d$variable == "Adaptation" & d$Gain == -0.3,c(1,2,5,6)],
             .(block), summarise, 
        N    = length(value),
        means = round(mean(value),2),
        sds = round(sd(value), 2),
        se   = sds / sqrt(N) )
adn2$type <-"AllTrials"

```


```{r}
adn <- rbind(adn1, adn2)
adn$type <- factor(adn$type)

adn$stars <- rep('*', 8)
adn$stars[2] <- ''
adn$y <- adn$means - adn$se - 0.02
adn$x <- as.numeric(adn$block) - 0.02
levels(adn$type) <- c("All Trials", "Test Trials")
adn$type <- factor(adn$type, levels = rev(levels(factor(adn$type))))

p <- ggplot(adn, aes(y = means, x=as.numeric(block))) +
     geom_bar(stat = "identity", width=0.5) +
     geom_errorbar(aes(ymin=means-se, ymax=means+se), width=.2) +
     geom_text(aes(x, y, label=stars, group=NULL), alpha=0.7, size=10) +
     facet_wrap(~type) +
     coord_flip()+ scale_x_reverse() +
     geom_abline(aes(intercept=0, slope=0)) +
     ylab("Saccadic Gain (Change from Baseline + Standard Error)") +
     xlab("Block") +
     ggtitle("Effect of Saccadic Adaptation")
     
p
pdf("fig_Adaptation.pdf", paper='USr', width=11)
p
dev.off()
     
```


```{r tbl:SA}
names(t)  <- c('t','df','p','mean')
names(tn) <- c('t','df','p','mean')
rownames(t) <- c('Block 1:','Block 2:','Block 3:','Block 4:')
rownames(tn) <- c('Block 1:','Block 2:','Block 3:','Block 4:')
t$t <- round(t$t,1)
tn$t <- round(tn$t,1)
t$p <- round(t$p, 2)
tn$p <- round(tn$p, 2)
t$mean <- round(t$mean, 2)
tn$mean <- round(tn$mean, 2)

require(gridExtra)
grid.arrange(tableGrob(t), tableGrob(tn), ncol=2, 
             sub="tbl:SA. (a) Test trials only, (b) All trials", clip=FALSE)


pdf("tbl.SA.pdf", paper='USr', width=11)
grid.arrange(tableGrob(t), tableGrob(tn), ncol=2, 
             sub="tbl:SA. (a) Test trials only, (b) All trials", clip=FALSE)
dev.off()

```


I did run 3 people with no gain, and 3 with the positive gain. Averages are
presented here (not enough to run any tests).
```{r}

t3 <- dcast(d, Sj ~ block + variable, 
            subset=.(variable == "Adaptation" & Gain == 0), fun=mean) 
#Not enough participants for t.test.mds
t3
dcast(d, . ~ variable, 
            subset=.(variable == "Adaptation" & Gain == 0), fun=mean) 


t4 <- dcast(d, Sj ~ block + variable, 
            subset=.(variable == "Adaptation" & Gain == 0.3), fun=mean) 
#Not enough participants for t.test.mds
t4
dcast(d, . ~ variable, 
            subset=.(variable == "Adaptation" & Gain == 0.3), fun=mean) 

#Note There are 4 sessions, and each of these are difference scores from baseline.

# Calculate the group means across the blocks

# Collapsing across blocks 1-4 (weighted?), different from zero?
t5 <- dcast(d, Sj ~ Gain + variable, 
            subset=.(variable == "Adaptation"), fun=mean) 
names(t5)
t.test.md(t5[[2]]) #low n
t.test.md(t5[[3]])
t.test.md(t5[[4]]) #low n



# Cannot viably compare gain conditions due to lack of random assignment and 
#   unequal n's.
```

So, as a group, those people in the reduction condition
significantly shortened their saccades both overall and in each
block (note that this is based on trial conditions as there were too few
test trials, considering how noisy the data was, to calculate anything
based on them). Note that in the 3 participants in the sham-adaptation condition,
the typical change was an order of magnitude smaller and varied in direciton (as 
would be expected).

```{r}
dcast(d, Sj ~ variable, 
            subset=.(variable == "Adaptation" & Gain == 0), fun=mean) 
```

For the sake of comparison of saccadic adaptation with landmark/line bisection
tasks, a single measure is required to identify participants as demonstrating
strong and reliable adaptation. Both magnitude, and reliability are important.
Magnitude is well measured by the median score (i.e., the average of the remaining
two blocks when the strongest and weakest are ignored). Reliability could be the
standard deviation, but that would penalize a particularily strong block eaqually
to a particularily weak block. 

For now, I suspect simply the mean of the 4 blocks will have to do. Don't know.


## Landmark and Line Bisection

The simplest thing to do is average across all the blocks. This loses information
though, so it's not the most ideal.

Rather than averaging, the slope of a lm could be used across the 4 (or 5) blocks.


```{r}
require(pwr)
d.test <- function(v){
  #Cohen's D for one-sample t.test.md
  return(mean(t6$Bisection, na.rm=TRUE) / var(t6$Bisection, na.rm=TRUE))
}

t6 <- dcast(d, Sj + Dir~ variable , 
            subset=.(Gain == -0.3 & Dir == 1), fun=mean) 
# Note: if -Dir is included ^, uncomment these 3 lines (no difference)
#summary(t6$Dir)
#t6$Bisection <- as.numeric(as.character(t6$Dir)) * t6$Bisection
#t6$Landmark  <- as.numeric(as.character(t6$Dir)) * t6$Landmark



# 3 particpants had no good adaptation data
t6 <- t6[!is.na(t6$Adaptation),]
summary(t6)
# Power to detect a "small" effect size.
pwr.t.test(d=0.2,n=length(t6$Landmark),sig.level=0.05,type="one.sample",alternative="two.sided")

t.test.md(t6$Landmark)
# Size of delta to have a 95% chance of detecting it.
power.t.test(n=length(t6$Landmark), sd=sd(t6$Landmark, na.rm=TRUE), power=0.95, type="one.sample")


t.test.md(t6$Bisection)
# Size of delta to have a 95% chance of detecting it.
power.t.test(n=length(t6$Bisection), sd=sd(t6$Bisection, na.rm=TRUE), power=0.95, type="one.sample")


cor.test(t6$Adaptation, t6$Landmark)
cor.test(t6$Adaptation, t6$Bisection)
cor.test(t6$Landmark, t6$Bisection)

t6b <- t6[t6$Adaptation < median(t6$Adaptation),] #looking at only the strongest adapters
median(t6$Adaptation)

t.test.md(t6b$Landmark)
t.test.md(t6b$Bisection)


# Repeat for the other gain:
t7 <- dcast(d, Sj +Dir ~ variable , 
            subset=.(Gain == 0.3), fun=mean) 
t7$Bisection <- as.numeric(as.character(t7$Dir)) * t7$Bisection
t7$Landmark  <- as.numeric(as.character(t7$Dir)) * t7$Landmark
t7
```

Correlations:
```{r}
cor.test(t6$Adaptation, t6$Bisection)
cor.test(t6$Adaptation, t6$Landmark)
```




Basically, there is absolutely no hint of an effect of saccadic reduction on
either landmark or line bisection tasks.




Need to graphically show how small the changes were. It should be obvious that
the effect would have to be tiny in order for it to for this to be a type-1
error (i.e, graphically show a power analysis)



Statistical effect size: The typical error in line bisection is Xcm, or 
x percent of a 10cm line. The maximum effect plausible effect of SA on 
line bisection, given the lack of significant findings, is YCM, or 1/Y the typical
error in locating the center of the line. (?? this is smaller than the 
precision of the touch screen??). 

From above: (lines ~85-89), typical error is 0.1510164 cm and 0.2342002 cm for
LB and LT respectively. The average bias is 0.008334 cm and 0.20156 cm for LB
and LT again.






